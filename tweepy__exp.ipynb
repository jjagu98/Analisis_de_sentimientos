{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUAF3F87WUYP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "import json\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "consumer_key= 'xxxxxxxx'\n",
        "consumer_secret= 'xxxxx'\n",
        "access_token= 'xxxxx'\n",
        "access_token_secret= 'xxxxx'\n",
        "\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth,\n",
        "                 wait_on_rate_limit=True,\n",
        "                 wait_on_rate_limit_notify=True)\n",
        "#Obtener informacion de un usario\n",
        "data = api.me()\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynO7t2w2WXbh"
      },
      "outputs": [],
      "source": [
        "# Authenticate to Twitter\n",
        "try:\n",
        "    api.verify_credentials()\n",
        "    print(\"Authentication OK\")\n",
        "except:\n",
        "    print(\"Error during authentication\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FURVrHUHWjCL"
      },
      "outputs": [],
      "source": [
        "search_results = api.search(q=\"Olympic Games\", count=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Djm_v_-bWk55"
      },
      "outputs": [],
      "source": [
        "def analyze_sentiment(tweet):\n",
        "        analysis = TextBlob(clean_tweet(tweet))\n",
        "        \n",
        "        if analysis.sentiment.polarity > 0:\n",
        "            return \"positive\"\n",
        "        elif analysis.sentiment.polarity == 0:\n",
        "            return \"neutral\"\n",
        "        else:\n",
        "            return \"negative\"\n",
        "\n",
        "\n",
        "def tweets_to_data_frame(tweets):\n",
        "        df = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])\n",
        "\n",
        "        df['id'] = np.array([tweet.id for tweet in tweets])\n",
        "        df['len'] = np.array([len(tweet.text) for tweet in tweets])\n",
        "        df['date'] = np.array([tweet.created_at for tweet in tweets])\n",
        "        df['source'] = np.array([tweet.source for tweet in tweets])\n",
        "        df['likes'] = np.array([tweet.favorite_count for tweet in tweets])\n",
        "        df['retweets'] = np.array([tweet.retweet_count for tweet in tweets])\n",
        "\n",
        "        return df\n",
        "def clean_tweet(tweet):\n",
        "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|(RT)\", \" \", tweet).split())\n",
        "def lower_tweet(tweet):\n",
        "      return tweet.lower()   \n",
        "twi=tweets_to_data_frame(search_results)\n",
        "twi['Tweets']=twi['Tweets'].apply(clean_tweet) \n",
        "twi['sentiment'] = np.array([analyze_sentiment(tweet) for tweet in twi['Tweets']])\n",
        "twi['Tweets']=twi['Tweets'].apply(lower_tweet)\n",
        "twi.head(10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfGE9u5eWpBI"
      },
      "outputs": [],
      "source": [
        "class multinomialNB:\n",
        "  def __init__(self, X_train, y_train,X_test):\n",
        "    self.D=X_train\n",
        "    self.C=y_train\n",
        "    self.test=X_test\n",
        "    # compute the priors\n",
        "    # convert the character class to numbers (easier to work with)\n",
        "  def fit(self):\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(self.C)\n",
        "    priors = np.bincount(y) / y.shape[0]\n",
        "    class_type = np.unique(y)\n",
        "    class_nums = class_type.shape[0]\n",
        "    feature_nums = self.D.shape[1]\n",
        "    likelihood = np.zeros((class_nums, feature_nums))\n",
        "\n",
        "    # compute the word likelihood p(w_t∣C)\n",
        "    # apply lapace smoothing\n",
        "    for index, output in enumerate(class_type):\n",
        "        subset = X_train_dtm[np.equal(y, output)]\n",
        "        likelihood[index, :] = (np.sum(subset, axis = 0) + 1) / (np.sum(subset) + feature_nums)\n",
        "    return priors,likelihood    \n",
        "  def predict(self,priors,likelihood):\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(self.C) \n",
        "    class_type = np.unique(y)\n",
        "    class_nums = class_type.shape[0]\n",
        "    # make prediction on test set\n",
        "    predictions = np.zeros(self.test.shape[0], dtype = np.int)\n",
        "    for index1, document in enumerate(self.test):\n",
        "        \n",
        "        # stores the p(C|D) for each class\n",
        "        posteriors = np.zeros(class_nums)\n",
        "\n",
        "        # compute p(C = k|D) for the document for all class\n",
        "        # and return the predicted class with the maximum probability\n",
        "        for c in range(class_nums):\n",
        "\n",
        "            # start with p(C = k)\n",
        "            posterior = np.log(priors[c])\n",
        "            likelihood_subset = likelihood[c, :]\n",
        "\n",
        "            # compute p(D∣C = k)\n",
        "            prob = document * np.log(likelihood_subset)\n",
        "            posterior += np.sum(prob)\n",
        "            posteriors[c] = posterior\n",
        "\n",
        "        # compute the maximum p(C|D)\n",
        "        prediction = np.argmax(posteriors)\n",
        "        predictions[index1] = prediction\n",
        "    \n",
        "    # convert the prediction to the original class label\n",
        "    predicted_class = le.inverse_transform(predictions)\n",
        "    return predicted_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HpykL1TWyP5"
      },
      "outputs": [],
      "source": [
        "X, y = twi['Tweets'], twi['sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXxDgJUxWzR0"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = X[:50], X[50:], y[:50], y[50:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9auCoGDW2hr"
      },
      "outputs": [],
      "source": [
        "vect = CountVectorizer()\n",
        "X_train_dtm = vect.fit_transform(X_train)\n",
        "X_test_dtm = vect.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmaJlYxvW7pK"
      },
      "outputs": [],
      "source": [
        "NB=multinomialNB(X_train_dtm,y_train,X_test_dtm)\n",
        "NBprob=NB.fit()\n",
        "prior=NBprob[0]\n",
        "likelihood=NBprob[1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqLTgDEVW8vR"
      },
      "outputs": [],
      "source": [
        "y_pred =NB.predict(prior,likelihood)\n",
        "y_train=np.array(y_train)\n",
        "y_test=np.array(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSMnBwKoXRC4"
      },
      "outputs": [],
      "source": [
        "#performance measures\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRJl0lPcXUsC"
      },
      "outputs": [],
      "source": [
        "#sklearn classifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "encoder = LabelEncoder()\n",
        "y_train_NB= encoder.fit_transform(y_train)\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train_dtm, y_train_NB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdtY976fXc0P"
      },
      "outputs": [],
      "source": [
        "y_pred_NB=classifier.predict(X_test_dtm) \n",
        "y_test_NB=encoder.fit_transform(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UCKwIo_XgfB"
      },
      "outputs": [],
      "source": [
        "#performance measures for sklearn model\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test_NB,y_pred_NB))\n",
        "print(classification_report(y_test_NB,y_pred_NB))\n",
        "print(accuracy_score(y_test_NB, y_pred_NB))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzMwZ-YKXoUp"
      },
      "outputs": [],
      "source": [
        "#roc curve for each class using a sklearn model\n",
        "from yellowbrick.classifier.rocauc import roc_auc\n",
        "roc_auc(classifier, X_train_dtm, y_train_NB, X_test=X_test_dtm, y_test=y_test_NB, classes=['negative','neutral','positive'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "tweepy _exp",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
